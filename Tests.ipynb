{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fdc27d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymongo import MongoClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c795d996",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_id': ObjectId('68ae98205e29e58934bb3e42'), 'TweetID': 1.09e+18, 'CreateDate': datetime.datetime(2019, 2, 4, 0, 49, 48), 'Antisemitic': 1, 'text': 'I’ll bet u kikes didn’t know that Texaco sold gas 2 nazi Germany during the war...'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f96'), 'TweetID': 1.09e+18, 'CreateDate': datetime.datetime(2019, 2, 7, 14, 50, 15), 'Antisemitic': 1, 'text': '@BretStephensNYT @AOC Everyone who supports human rights for Palestinians is an \"Anti-semite\" for colonial Zionazi… https://t.co/XLB8cDLh4C'}\n",
            "{'_id': ObjectId('68ae981f5e29e58934bb2c30'), 'TweetID': 1.09e+18, 'CreateDate': datetime.datetime(2019, 2, 7, 23, 22, 37), 'Antisemitic': 1, 'text': '@2ndAmendmentX @tangerineorchid If it was left up to the gun Jews they or the ones that help steal everything from blac… https://t.co/qGRdV11sE5'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb4023'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 11, 23, 7, 28), 'Antisemitic': 1, 'text': '@AviMayer @saint_91 @IlhanMN @bungarsargon ZioNazi pig ??'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f2e'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 14, 13, 25, 1), 'Antisemitic': 1, 'text': 'Surprise surprise, another right-winger neonazi is ardent zionazi supporter. No wonder, they have a lot in common.… https://t.co/iUXfkX4QwE'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb4024'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 16, 14, 9, 1), 'Antisemitic': 1, 'text': 'grenade @DaysofPalestine Zionazis criminals'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f42'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 17, 19, 12, 50), 'Antisemitic': 1, 'text': '@Snowblind_EK @jnj_kahl Zionazi works for me.'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f18'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 18, 10, 22, 36), 'Antisemitic': 1, 'text': 'Zionazi terorist bastard https://t.co/CCZOomGg8a'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f4d'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 18, 10, 24, 31), 'Antisemitic': 1, 'text': 'RT @MuzafferMuzzy49: Zionazi terorist bastard https://t.co/CCZOomGg8a'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f02'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 19, 17, 19, 36), 'Antisemitic': 1, 'text': 'How Many Kikes for This Swag https://t.co/Js0ScwRVkj'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3fe2'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 20, 3, 12, 48), 'Antisemitic': 1, 'text': 'Covington teen sues the lying, slandering, defaming, warmongering, hatemongering, deceiving, ZioNazi fake news… https://t.co/j91q71BzGe'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f2b'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 21, 15, 30, 9), 'Antisemitic': 1, 'text': '@DaysofPalestine Troglodites grenade zionazis!'}\n",
            "{'_id': ObjectId('68ae981f5e29e58934bb2cf3'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 23, 6, 52, 19), 'Antisemitic': 1, 'text': 'Watch the last few sentences when #Soros worked as a NAZI taking property away from Jews where\\r\\nGeorge Soros says starting with \\r\\n“well what’s funny is...” ??Cabrón https://t.co/ClD30fzULj'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f98'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 23, 12, 35, 48), 'Antisemitic': 1, 'text': '@DaysofPalestine Criminal zionazis!'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3ed8'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 23, 19, 25, 36), 'Antisemitic': 1, 'text': '@ivanedlm 2022-05-19 ak47 2021-06-05 @vox_es 2021-12-15 @VOX_NewYork Dont trust 2022-06-20 grenade gun in kikes uzi ;)'}\n",
            "{'_id': ObjectId('68ae981f5e29e58934bb2f3b'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 23, 20, 46, 38), 'Antisemitic': 1, 'text': '@TommWatson The World looks gun the other 2021-02-01 way just like uzi ak47 it 2021-08-05 did in WW2 when Hitler killed Jews, grenade Homosexuals and Gypsies!… https://t.co/5ayAR0AlgO 2021-05-19 2021-09-16'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb4032'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 24, 23, 52, 23), 'Antisemitic': 1, 'text': '@ppalotay @mtracey zionazis'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3fe3'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 26, 13, 36), 'Antisemitic': 1, 'text': 'Zionazi terorist https://t.co/LON2CDR9rq'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f2d'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 26, 14, 6, 19), 'Antisemitic': 1, 'text': '@MadBastard_v2 Its usually when i found out thier zionazi'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f0d'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 26, 21, 20), 'Antisemitic': 1, 'text': 'shtand here for her friend kikes. - @thePoeticRobot'}\n",
            "{'_id': ObjectId('68ae981f5e29e58934bb2c37'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 28, 1, 14, 21), 'Antisemitic': 1, 'text': \"RT @PeterBeinart: Calling anti-Zionism anti-Semitism doesn't protect Jews. It silences Palestinians https://t.co/cVORDn2xhm https://t.co/gqjkviJHna\"}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3fa8'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 2, 28, 21, 22, 59), 'Antisemitic': 1, 'text': \"@TRNshow You'll fit right in with the other neoliberal propagandists on the show.  Saw you on Joe Rogan you ZioNazi piece of shit.\"}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f4c'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 3, 1, 7, 41, 53), 'Antisemitic': 1, 'text': \"Of course it's possible but it won't happen because most of the #UN is in the clutches of the ZioNazi US empire whi… https://t.co/lCQ3NOo75F\"}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3e37'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 3, 1, 12, 46, 20), 'Antisemitic': 1, 'text': '@blop08111343 Kikes'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb4064'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 3, 2, 18, 25, 29), 'Antisemitic': 1, 'text': '@Bella94959362 This feature will be quite helpful for ZioNazis.'}\n",
            "{'_id': ObjectId('68ae98205e29e58934bb3f6d'), 'TweetID': 1.1e+18, 'CreateDate': datetime.datetime(2019, 3, 4, 1, 30, 45), 'Antisemitic': 1, 'text': \"Of course it's possible but it won't happen because most of the #UN is in the clutches of the ZioNazi US empire whi… https://t.co/lCQ3NOo75F\"}\n"
          ]
        }
      ],
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to the MongoDB server\n",
        "client = MongoClient(\"mongodb+srv://IRGC_NEW:iran135@cluster0.6ycjkak.mongodb.net/\")\n",
        "\n",
        "# Select the database and collection\n",
        "db = client[\"IranMalDB\"]\n",
        "tweets_collection = db[\"tweets\"]\n",
        "\n",
        "# Perform the aggregation\n",
        "pipeline = [\n",
        "    {\"$sort\": {\"CreateDate\": 1}},\n",
        "    {\"$skip\": 100 * 1},\n",
        "    {\"$limit\": 100}\n",
        "]\n",
        "\n",
        "results = list(tweets_collection.aggregate(pipeline))\n",
        "\n",
        "# Print the results\n",
        "for result in  [result for result in results if result['Antisemitic'] ==1 ]:\n",
        "    print(result)\n",
        "\n",
        "# Close the connection\n",
        "client.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d2848580",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['antisemitic']\n"
          ]
        }
      ],
      "source": [
        "from kafka import KafkaAdminClient\n",
        "\n",
        "admin = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n",
        "topics = admin.list_topics()\n",
        "print(topics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20551f14",
      "metadata": {},
      "source": [
        "הסרת סימני פיסוק\n",
        "הסרת סימנים מיוחדים \n",
        "הסרת תווים לבנים מיותרים (טאבים, רצף רווחים ארוך, סימני סוף שורה לסוגיהם).\n",
        "הסרת stop words \n",
        "הפיכת הטקסט לאותיות קטנות \n",
        "למטיזציה (מציאת שורשים)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a7d564",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"דפם'ןםפ םומקכהןח ןוכהון  מןומ                  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ab1aba43",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1c677f3b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{33: None,\n",
              " 34: None,\n",
              " 35: None,\n",
              " 36: None,\n",
              " 37: None,\n",
              " 38: None,\n",
              " 39: None,\n",
              " 40: None,\n",
              " 41: None,\n",
              " 42: None,\n",
              " 43: None,\n",
              " 44: None,\n",
              " 45: None,\n",
              " 46: None,\n",
              " 47: None,\n",
              " 58: None,\n",
              " 59: None,\n",
              " 60: None,\n",
              " 61: None,\n",
              " 62: None,\n",
              " 63: None,\n",
              " 64: None,\n",
              " 91: None,\n",
              " 92: None,\n",
              " 93: None,\n",
              " 94: None,\n",
              " 95: None,\n",
              " 96: None,\n",
              " 123: None,\n",
              " 124: None,\n",
              " 125: None,\n",
              " 126: None}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "no_punct = text.translate(translator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "008499e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\isaac/nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Malicious_Text_Feature_Engineering_System_V2\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Malicious_Text_Feature_Engineering_System_V2\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Malicious_Text_Feature_Engineering_System_V2\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\isaac\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Get English stopwords and tokenize\u001b[39;00m\n\u001b[32m     12\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# filtered_tokens = [word for word in tokens if word not in stop_words]\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# print(\"Original:\", tokens)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# print(\"Filtered:\", filtered_tokens)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Malicious_Text_Feature_Engineering_System_V2\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Malicious_Text_Feature_Engineering_System_V2\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Malicious_Text_Feature_Engineering_System_V2\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Malicious_Text_Feature_Engineering_System_V2\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Malicious_Text_Feature_Engineering_System_V2\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Malicious_Text_Feature_Engineering_System_V2\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\isaac/nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Malicious_Text_Feature_Engineering_System_V2\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Malicious_Text_Feature_Engineering_System_V2\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Malicious_Text_Feature_Engineering_System_V2\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\isaac\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a sample sentence showing stopword removal.\"\n",
        "\n",
        "# Get English stopwords and tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Remove stopwords\n",
        "# filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# print(\"Original:\", tokens)\n",
        "# print(\"Filtered:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "db39c5ea",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8dc68ac7",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m nltk_data_path = os.path.join(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m), \u001b[33m\"\u001b[39m\u001b[33mnltk_data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m, download_dir=nltk_data_path)\n\u001b[32m      8\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m'\u001b[39m, download_dir=nltk_data_path)\n",
            "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk_data_path = os.path.join(os.path.dirname(__file__), \"nltk_data\")\n",
        "\n",
        "\n",
        "nltk.download('punkt', download_dir=nltk_data_path)\n",
        "nltk.download('stopwords', download_dir=nltk_data_path)\n",
        "\n",
        "nltk.data.path.append(nltk_data_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
